import chromadb
import time
import psutil
import os
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import argparse

# Classe principale pour le benchmark de ChromaDB
class ChromaDBBenchmark:
    def __init__(self):
        # Liste de documents √† indexer (m√©lange nature/techno)
        self.documents = [
            "Le ciel est bleu et d√©gag√© aujourd'hui",
            "Le soleil brille de mille feux dor√©s",
            "La mer scintille sous les rayons du soleil",
            "Les oiseaux chantent dans les arbres verts",
            "Le vent souffle doucement dans les feuilles",
            "Les fleurs embaument le jardin printanier",
            "La montagne se dresse majestueuse",
            "Le lac refl√®te les nuages blancs",
            "La for√™t regorge de vie sauvage",
            "L'herbe pousse dans la prairie verdoyante",
            "La technologie moderne transforme notre soci√©t√©",
            "L'intelligence artificielle r√©volutionne l'informatique",
            "Les algorithmes d'apprentissage automatique sont puissants",
            "Le machine learning analyse des donn√©es complexes",
            "Les r√©seaux de neurones simulent le cerveau humain",
            "La programmation Python est tr√®s populaire",
            "Les bases de donn√©es stockent des informations",
            "Le cloud computing offre des services distants",
            "L'analyse de donn√©es r√©v√®le des tendances cach√©es",
            "Les syst√®mes distribu√©s g√®rent la charge"
        ]
        # Requ√™tes de recherche √† tester
        self.queries = [
            "nature et paysage",
            "technologie et informatique",
            "ciel et m√©t√©o",
            "intelligence artificielle"
        ]
        # Dictionnaire pour stocker les fonctions d'embedding
        self.embedding_functions = {}
        # Dictionnaire pour stocker les r√©sultats du benchmark
        self.results = {}

    def setup_embedding_functions(self):
        """Configure les diff√©rentes fonctions d'embedding √† tester"""
        print("Configuration des fonctions d'embedding...")
        # Embedding par d√©faut de ChromaDB (souvent TF-IDF ou similaire)
        self.embedding_functions['default'] = None
        # SBERT MiniLM (rapide, s√©mantique)
        self.embedding_functions['sbert_mini'] = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        # SBERT Base (plus lourd, potentiellement plus pr√©cis)
        self.embedding_functions['sbert_base'] = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-mpnet-base-v2"
        )
        # OpenAI embedding (optionnel, n√©cessite une cl√© API)
        try:
            self.embedding_functions['openai'] = embedding_functions.OpenAIEmbeddingFunction(
                api_key="dummy_key"  # Remplacer par une vraie cl√© API si besoin
            )
        except:
            print("OpenAI embedding non disponible (cl√© API manquante)")

    def measure_memory_usage(self):
        """Mesure l'utilisation m√©moire actuelle du processus (en MB)"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024  # MB

    def benchmark_embedding_function(self, name, embedding_func):
        """Benchmark une fonction d'embedding sp√©cifique"""
        print(f"\n=== Benchmark: {name} ===")
        # Mesure m√©moire avant l'op√©ration
        memory_before = self.measure_memory_usage()
        # Cr√©ation du client ChromaDB
        client = chromadb.Client()
        # Suppression de la collection si elle existe d√©j√†
        try:
            client.delete_collection(name=f"benchmark_{name}")
        except:
            pass
        start_time = time.time()
        # Cr√©ation de la collection avec ou sans embedding personnalis√©
        if embedding_func:
            collection = client.create_collection(
                name=f"benchmark_{name}",
                embedding_function=embedding_func
            )
        else:
            collection = client.create_collection(name=f"benchmark_{name}")
        # Ajout des documents et mesure du temps
        add_start = time.time()
        collection.add(
            ids=[f"doc_{i}" for i in range(len(self.documents))],
            documents=self.documents
        )
        add_time = time.time() - add_start
        # Mesure m√©moire apr√®s ajout
        memory_after = self.measure_memory_usage()
        memory_usage = memory_after - memory_before
        # Mesure du temps de requ√™te pour chaque query
        query_times = []
        query_results = []
        for query in self.queries:
            query_start = time.time()
            results = collection.query(
                query_texts=[query],
                n_results=3
            )
            query_time = time.time() - query_start
            query_times.append(query_time)
            query_results.append(results)
        total_time = time.time() - start_time
        avg_query_time = sum(query_times) / len(query_times)
        # Stockage des r√©sultats pour ce mod√®le
        self.results[name] = {
            'total_time': total_time,
            'add_time': add_time,
            'avg_query_time': avg_query_time,
            'memory_usage': memory_usage,
            'query_results': query_results
        }
        # Affichage des m√©triques principales
        print(f"Temps total: {total_time:.3f}s")
        print(f"Temps d'ajout: {add_time:.3f}s")
        print(f"Temps de requ√™te moyen: {avg_query_time:.3f}s")
        print(f"Utilisation m√©moire: {memory_usage:.1f}MB")
        # Exemple de r√©sultats pour la premi√®re requ√™te
        print(f"\nExemple de r√©sultats pour '{self.queries[0]}':")
        for i, doc in enumerate(query_results[0]['documents'][0]):
            distance = query_results[0]['distances'][0][i]
            print(f"  {i+1}. {doc[:50]}... (distance: {distance:.3f})")

    def run_benchmark(self):
        """Ex√©cute le benchmark complet sur tous les embeddings"""
        print("=== BENCHMARK CHROMADB AVEC DIFF√âRENTS EMBEDDINGS ===")
        print(f"Nombre de documents: {len(self.documents)}")
        print(f"Nombre de requ√™tes: {len(self.queries)}")
        self.setup_embedding_functions()
        # Boucle sur chaque embedding √† tester
        for name, func in self.embedding_functions.items():
            if name == 'openai' and func is None:
                continue  # Ignore si OpenAI non dispo
            try:
                self.benchmark_embedding_function(name, func)
            except Exception as e:
                print(f"Erreur avec {name}: {e}")
        self.display_comparison()

    def display_comparison(self):
        """Affiche un tableau comparatif des r√©sultats du benchmark"""
        print("\n" + "="*80)
        print("COMPARAISON DES PERFORMANCES")
        print("="*80)
        # En-t√™tes du tableau
        print(f"{'Mod√®le':<15} {'Temps total':<12} {'Ajout':<8} {'Requ√™te':<9} {'M√©moire':<10}")
        print("-" * 80)
        # Affichage des r√©sultats pour chaque embedding
        for name, results in self.results.items():
            print(f"{name:<15} {results['total_time']:<12.3f} {results['add_time']:<8.3f} "
                  f"{results['avg_query_time']:<9.3f} {results['memory_usage']:<10.1f}")
        # Recommandations automatiques
        print("\n" + "="*80)
        print("RECOMMANDATIONS")
        print("="*80)
        if 'default' in self.results and 'sbert_mini' in self.results:
            default_time = self.results['default']['total_time']
            sbert_time = self.results['sbert_mini']['total_time']
            if sbert_time < default_time * 1.2:  # SBERT pas trop lent
                print("‚úÖ SBERT recommand√© pour une meilleure qualit√© s√©mantique")
            else:
                print("‚ö° Embedding par d√©faut recommand√© pour la vitesse")
        # Affichage du plus rapide et du moins gourmand en m√©moire
        fastest = min(self.results.items(), key=lambda x: x[1]['total_time'])
        print(f"üèÉ Plus rapide: {fastest[0]} ({fastest[1]['total_time']:.3f}s)")
        lowest_memory = min(self.results.items(), key=lambda x: x[1]['memory_usage'])
        print(f"üíæ Moins de m√©moire: {lowest_memory[0]} ({lowest_memory[1]['memory_usage']:.1f}MB)")

# Fonction principale pour lancer le benchmark via la ligne de commande

def main():
    parser = argparse.ArgumentParser(description='Benchmark ChromaDB avec diff√©rents embeddings')
    parser.add_argument('--save', action='store_true', help='Sauvegarder les r√©sultats')
    args = parser.parse_args()
    benchmark = ChromaDBBenchmark()
    benchmark.run_benchmark()
    # Option de sauvegarde des r√©sultats dans un fichier texte
    if args.save:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"benchmark_results_{timestamp}.txt"
        with open(filename, 'w') as f:
            f.write("=== R√âSULTATS DU BENCHMARK CHROMADB ===\n\n")
            for name, results in benchmark.results.items():
                f.write(f"{name}:\n")
                f.write(f"  Temps total: {results['total_time']:.3f}s\n")
                f.write(f"  Temps d'ajout: {results['add_time']:.3f}s\n")
                f.write(f"  Temps de requ√™te moyen: {results['avg_query_time']:.3f}s\n")
                f.write(f"  Utilisation m√©moire: {results['memory_usage']:.1f}MB\n\n")
        print(f"\nR√©sultats sauvegard√©s dans {filename}")

# Point d'entr√©e du script
if __name__ == "__main__":
    main()